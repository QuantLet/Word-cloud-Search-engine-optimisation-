{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations \n",
    "from itertools import permutations\n",
    "from itertools import chain\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "courseletlist = (\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F175%2F163757167020201210_Liu_crypto_p2p_lending.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F272%2F1654160257Lesson1-1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F273%2F1654160288Lesson1-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F274%2F1654160327Lesson1-3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F275%2F1654160374Lesson1-4.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F276%2F1654160475Lesson1-5.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F277%2F1654160518Lesson1-6.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F279%2F1654251498Lesson2-1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F278%2F1654160549Lesson1-7.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F280%2F1654251511Lesson2-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F281%2F1654251525Lesson2-3.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F103%2F20210303+IA+METIS+Reinforcement+Learning.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F107%2F1636712642CATE_meets_ML_Presentation.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F120%2F163458263420190429+Hae+Ni+LDA+DTM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F120%2F163646337920210921+Hae+Ni+LDA.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F121%2F163646358220210708+Hae+Ni+LDA+extensions.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F108%2F163595835420210530+METIS+WANG+Kalman+Filter.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F110%2F1632126441nodalida2021_summaryQuality_slides.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F112%2F163664661320211013+Ren+LI+Hae+Expectile+FRM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F114%2F1635233254Shapley.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F118%2F1636625638FRM%40EM.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F119%2F163231165920210324+Wan+Hae+Li+k-expectile+clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F130%2F1633104997PAC.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F134%2F163368764620210923+Mer+Hae+GAN+Generative+Adversarial+Networks.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F139%2F163402565120211012+Kho+Hae+Trespassing+random+forests+with+a+pointed+stick+for+self+defence.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F217%2F1644582711Berlin_short_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F235%2F1649426301Variable+importance+measures+for+RF+.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F179%2F16376558632021122+SBA+JW+Hae+EPF++Quantinar.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F186%2F1645194357Presentation_Quantinar_with_videos.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F306%2FBarHan2021_talk.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F181%2F163826827620211130+LI+Hae+Case+based+Bancruptcy+prediction.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F224%2F164728525020220305+LI+Electricity+Market+Coupling.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F196%2F1642599075163458186420200403+METIS+Kho+Hae+Spectral+Clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F198%2F1642599289163231165920210324+Wan+Hae+Li+k-expectile+clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F141%2F163527012720210526+SAE+NAG+HAE+SIZ+Understanding+jumps+in+high+frequency+digital+asset+markets.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F201%2F16426862241636625638FRM%40EM_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F202%2F1642686343163774999520210912+Hae+Li+Tao+Dynamic+Crypto+Networks_course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F189%2F163958119120211130+Hae+Wan+Kot+ComputerMuseum.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F183%2F1643806658KDE+ill-posed+problems.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F167%2F163699399420211115+Liu+Word+Embeddings+2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F157%2F1642778303introduction_data_science.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F180%2F16377653596.+model+assessment+-+part+4+-+appendix.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F128%2F163707553520210331_METIS_Hel_GANs_for_Time_Series.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F127%2F163458156320190528+Cea+Hae+Scagnostics.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F117%2F163458186420200403+METIS+Kho+Hae+Spectral+Clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F106%2F163458175020200914+Hae+DS2+Data+Science+%26+Digital+Society.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F190%2F1640038524Instruction+for+Creating+Quantlets.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F144%2F1636624210NNCSR_Slides.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F184%2F163888970320211207+Zin+Reu+Hae+USC+Quantinar+40+min+PDF.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F133%2F163707963820210525_Hae_Xia_Crypto_Indices-2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F129%2F163458163120200915+Kim+Hae+Tri+VCRIX.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F123%2F163664570620210922+Mat+Pac+Hae+guide+hedging+CC.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F122%2F163283000220210923+Cul+Hae+Pet+Xia+Cryptocurrency+as+an+asset+class.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F116%2F163774999520210912+Hae+Li+Tao+Dynamic+Crypto+Networks.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F113%2F1632580703FRM+for+Cryptos.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F101%2F163170747120210914+Reu+DSF+Digital+Surrogate+Finance+Doc.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F97%2F163458105020210808+METIS+Win+Pricing+Kernel+Risk+Premium.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F96%2F163299370720210908_CRC21_Hae_Rodeo_or_Ascot.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F96%2F1635155323202109_RoA.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F79%2F164604019720210502+Hae+Har+Reu+Understanding+CCs.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F142%2F163627928020211107+Hae+Iva+Mat+Delaunay+Triangulation_A_Shape.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F135%2F1649084960Chapter+1.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F138%2F1649094430Chapter+4.pdf',\n",
    "# problematic:\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F197%2F1642599236164171878820211207+Hae+Zin+Hierarchical+Clustering+course.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F241%2F1650632942Biographical+Background+Information.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F210%2F164390414020220130+METIS+Gua+Hae+Model+Selection+Criteria.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F194%2F164171878820211207+Hae+Zin+Hierarchical+Clustering.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F170%2F163709451820211117+Hae+Qia+Network+Centrality.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F136%2F1649094328Chapter+2.pdf',\n",
    "'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F137%2F1633950248Chapter+3.pdf'\n",
    ")\n",
    "\n",
    "ids = ([\"id-{}\".format(i+1) for i in range(len(courseletlist))])\n",
    "\n",
    "\n",
    "stopwords_list = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"you're\",\"you've\",\"you'll\",\"you'd\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"she's\",\"her\",\"hers\",\"herself\",\"it\",\"it's\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",'that',\"that'll\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"don't\",\"should\",\"should've\",\"now\",\"d\",\"ll\",\"m\",\"o\",\"re\",\"ve\",\"y\",\"ain\",\"aren\",\"aren't\",\"couldn\",\"couldn't\",\"didn\",\"didn't\",\"doesn\",\"doesn't\",\"hadn\",\"hadn't\",\"hasn\",\"hasn't\",\"haven\",\"haven't\",\"isn\",\"isn't\",\"ma\",\"mightn\",\"mightn't\",\"mustn\",\"mustn't\",\"needn\",\"needn't\",\"shan\",\"shan't\",\"shouldn\",\"shouldn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\"}\n",
    "\n",
    "id_matrix = pd.DataFrame({'id': ids, 'url': courseletlist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_matrix = id_matrix.loc[1:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(file_name, url):\n",
    "    '''Download a PDF file with an URL (Step 1)'''\n",
    "    # Define HTTP Headers\n",
    "    headers = {\"User-Agent\": \"Chrome/51.0.2704.103\"}\n",
    "    # Download image\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # if response is OK download the PDF and store it, else write the status\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_string(file_name):\n",
    "    '''Transform a PDF file to a list of string pages (Step 2)'''\n",
    "    # opening the file\n",
    "    imported_pdf = open(file_name, 'rb')\n",
    "    # convert PDF to readable file\n",
    "    transformed_pdf = PyPDF2.PdfFileReader(imported_pdf)\n",
    "    # get number of pages\n",
    "    totalpages = transformed_pdf.numPages\n",
    "    # read the data and store in a list\n",
    "    pdf_output = [transformed_pdf.getPage(i) for i in range(totalpages)]\n",
    "    # extract result\n",
    "    pdf_output = [pdf_output[i].extractText() for i in range(totalpages)]\n",
    "    return pdf_output, totalpages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old\n",
    "# def cleaning(file_name):\n",
    "#     '''Initial PDF cleaning procedure (Step 3)'''\n",
    "#     pdf_output, totalpages = create_string(file_name)\n",
    "#     # # cleaning URLs\n",
    "#     pdf_output = [re.sub(pattern = \"http[^ ]*\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     # # cleaning symbols\n",
    "#     pdf_output = [re.sub(pattern = \"\\\\n\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     pdf_output = [re.sub(pattern = \"\\W|\\d\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     pdf_output = [re.sub(pattern = \"[^a-zA-Z]\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     # # cleaning multispaces\n",
    "#     pdf_output = [re.sub(pattern = \"\\s{2,}\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     # # cleaning out 1-2-worders\n",
    "#     pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "#     # # lower-casing\n",
    "#     pdf_output = [pdf_output[i].lower() for i in range(totalpages)]\n",
    "#     pdf_output =  [[ps.stem(word) for word in sentence.split(\" \")] for sentence in pdf_output]\n",
    "#     return pdf_output, totalpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(file_name):\n",
    "    '''Initial PDF cleaning procedure (Step 3)'''\n",
    "    pdf_output, totalpages = create_string(file_name)\n",
    "    # # cleaning URLs\n",
    "    pdf_output = [re.sub(pattern = \"http[^ ]*\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # cleaning symbols\n",
    "    pdf_output = [re.sub(pattern = \"\\\\n\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \"\\W|\\d\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \"[^a-zA-Z]\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # cleaning multispaces\n",
    "    pdf_output = [re.sub(pattern = \"\\s{2,}\", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # cleaning out 1-2-worders\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    pdf_output = [re.sub(pattern = \" .{1,2} \", repl = \" \", string = pdf_output[i]) for i in range(totalpages)]\n",
    "    # # lower-casing\n",
    "    pdf_output = [pdf_output[i].lower() for i in range(totalpages)]\n",
    "    pdf_output = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in pdf_output]\n",
    "    pdf_output = [' '.join(pdf_output[i]) for i in range(len(pdf_output))]\n",
    "    return pdf_output, totalpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lists(file_name):\n",
    "    '''Creating the base one-word, two-word and three-word lists, the permutation lists for two- and three-word lists (Step 4)'''\n",
    "    pdf_output, totalpages = cleaning(file_name)\n",
    "    # split to a list\n",
    "    word_list = [pdf_output[i].split(\" \") for i in range(totalpages)]\n",
    "    # stemming\n",
    "    word_list_stemmed = [[ps.stem(word_list[i][j]) for j in range(len(word_list[i]))] for i in range(totalpages)]    \n",
    "    word_list_stemmed = pd.DataFrame(word_list_stemmed)\n",
    "    # one-word section\n",
    "    one_word_list = [word_list_stemmed.iloc[j, i] for j in range(totalpages) for i in range(word_list_stemmed.shape[1])]\n",
    "  \n",
    "    one_word_list = [x for x in one_word_list if x not in stopwords_list]\n",
    "\n",
    "    # two-word section\n",
    "    two_word_list = [[word_list_stemmed.iloc[j, i], word_list_stemmed.iloc[j, i+1]] for j in range(totalpages)  for i in range(word_list_stemmed.shape[1] - 1)]\n",
    "    two_word_permutation_list = [[p for p in permutations(two_word_list[k])][1:] for k in range(len(two_word_list))]\n",
    "    two_word_permutation_set = set(list(chain(*two_word_permutation_list)))\n",
    "    two_word_permutation_set = pd.DataFrame(two_word_permutation_set)\n",
    "\n",
    "    # three-word section\n",
    "    three_word_list = [[word_list_stemmed.iloc[j, i], word_list_stemmed.iloc[j, i+1], word_list_stemmed.iloc[j, i+2]] for j in range(totalpages) for i in range(word_list_stemmed.shape[1] - 2)]\n",
    "    three_word_permutation_list = [[p for p in permutations(three_word_list[k])][1:] for k in range(len(three_word_list))]\n",
    "    three_word_permutation_set = set(list(chain(*three_word_permutation_list)))\n",
    "    three_word_permutation_set = pd.DataFrame(three_word_permutation_set)\n",
    "\n",
    "    return word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurance_one_matrix_creator(file_name):\n",
    "    '''Creating the occurrance matrix for one-word combinations (Step 7)'''\n",
    "    word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(file_name)\n",
    "    # copying the data\n",
    "    words = one_word_list.copy()\n",
    "    # creating the three-word combinations as one string\n",
    "    words = [x for x in words if x != \"\"]\n",
    "    words = [x for x in words if x != \" \"]    \n",
    "    # crating the dictionary\n",
    "    dictionary_one_word = {}\n",
    "    \n",
    "    # counting word occurances\n",
    "    for word in words:\n",
    "        if word in dictionary_one_word:\n",
    "            dictionary_one_word[word] = dictionary_one_word[word] + 1\n",
    "        else:\n",
    "            dictionary_one_word[word] = 1\n",
    "    dictionary_one_word = list(dictionary_one_word)\n",
    "    dictionary_one_word = list(filter(None, dictionary_one_word))\n",
    "    return dictionary_one_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdf(ids[0], courseletlist[0])\n",
    "\n",
    "pdf_output, totalpages = cleaning(ids[0])\n",
    "\n",
    "word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(ids[0])\n",
    "\n",
    "dictionary_one_word = occurance_one_matrix_creator(ids[0])\n",
    "\n",
    "combined_pdf = [' '.join(pdf_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'15' b'0'\n",
      "Superfluous whitespace found in object header b'44' b'0'\n",
      "Superfluous whitespace found in object header b'48' b'0'\n",
      "Superfluous whitespace found in object header b'52' b'0'\n",
      "Superfluous whitespace found in object header b'55' b'0'\n",
      "Superfluous whitespace found in object header b'14' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'13' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'43' b'0'\n",
      "Superfluous whitespace found in object header b'24' b'0'\n",
      "Superfluous whitespace found in object header b'23' b'0'\n",
      "Superfluous whitespace found in object header b'22' b'0'\n",
      "Superfluous whitespace found in object header b'32' b'0'\n",
      "Superfluous whitespace found in object header b'31' b'0'\n",
      "Superfluous whitespace found in object header b'30' b'0'\n",
      "Superfluous whitespace found in object header b'41' b'0'\n",
      "Superfluous whitespace found in object header b'40' b'0'\n",
      "Superfluous whitespace found in object header b'39' b'0'\n",
      "Superfluous whitespace found in object header b'42' b'0'\n",
      "Superfluous whitespace found in object header b'16' b'0'\n",
      "Superfluous whitespace found in object header b'33' b'0'\n",
      "Superfluous whitespace found in object header b'47' b'0'\n",
      "Superfluous whitespace found in object header b'46' b'0'\n",
      "Superfluous whitespace found in object header b'45' b'0'\n",
      "Superfluous whitespace found in object header b'51' b'0'\n",
      "Superfluous whitespace found in object header b'50' b'0'\n",
      "Superfluous whitespace found in object header b'49' b'0'\n",
      "Superfluous whitespace found in object header b'54' b'0'\n",
      "Superfluous whitespace found in object header b'53' b'0'\n",
      "Superfluous whitespace found in object header b'91' b'0'\n",
      "Superfluous whitespace found in object header b'63' b'0'\n",
      "Superfluous whitespace found in object header b'62' b'0'\n",
      "Superfluous whitespace found in object header b'61' b'0'\n",
      "Superfluous whitespace found in object header b'89' b'0'\n",
      "Superfluous whitespace found in object header b'88' b'0'\n",
      "Superfluous whitespace found in object header b'87' b'0'\n",
      "Superfluous whitespace found in object header b'90' b'0'\n",
      "Superfluous whitespace found in object header b'81' b'0'\n",
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'15' b'0'\n",
      "Superfluous whitespace found in object header b'44' b'0'\n",
      "Superfluous whitespace found in object header b'48' b'0'\n",
      "Superfluous whitespace found in object header b'52' b'0'\n",
      "Superfluous whitespace found in object header b'55' b'0'\n",
      "Superfluous whitespace found in object header b'14' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'13' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'43' b'0'\n",
      "Superfluous whitespace found in object header b'24' b'0'\n",
      "Superfluous whitespace found in object header b'23' b'0'\n",
      "Superfluous whitespace found in object header b'22' b'0'\n",
      "Superfluous whitespace found in object header b'32' b'0'\n",
      "Superfluous whitespace found in object header b'31' b'0'\n",
      "Superfluous whitespace found in object header b'30' b'0'\n",
      "Superfluous whitespace found in object header b'41' b'0'\n",
      "Superfluous whitespace found in object header b'40' b'0'\n",
      "Superfluous whitespace found in object header b'39' b'0'\n",
      "Superfluous whitespace found in object header b'42' b'0'\n",
      "Superfluous whitespace found in object header b'16' b'0'\n",
      "Superfluous whitespace found in object header b'33' b'0'\n",
      "Superfluous whitespace found in object header b'47' b'0'\n",
      "Superfluous whitespace found in object header b'46' b'0'\n",
      "Superfluous whitespace found in object header b'45' b'0'\n",
      "Superfluous whitespace found in object header b'51' b'0'\n",
      "Superfluous whitespace found in object header b'50' b'0'\n",
      "Superfluous whitespace found in object header b'49' b'0'\n",
      "Superfluous whitespace found in object header b'54' b'0'\n",
      "Superfluous whitespace found in object header b'53' b'0'\n",
      "Superfluous whitespace found in object header b'91' b'0'\n",
      "Superfluous whitespace found in object header b'63' b'0'\n",
      "Superfluous whitespace found in object header b'62' b'0'\n",
      "Superfluous whitespace found in object header b'61' b'0'\n",
      "Superfluous whitespace found in object header b'89' b'0'\n",
      "Superfluous whitespace found in object header b'88' b'0'\n",
      "Superfluous whitespace found in object header b'87' b'0'\n",
      "Superfluous whitespace found in object header b'90' b'0'\n",
      "Superfluous whitespace found in object header b'81' b'0'\n",
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'15' b'0'\n",
      "Superfluous whitespace found in object header b'44' b'0'\n",
      "Superfluous whitespace found in object header b'48' b'0'\n",
      "Superfluous whitespace found in object header b'52' b'0'\n",
      "Superfluous whitespace found in object header b'55' b'0'\n",
      "Superfluous whitespace found in object header b'14' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'13' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'43' b'0'\n",
      "Superfluous whitespace found in object header b'24' b'0'\n",
      "Superfluous whitespace found in object header b'23' b'0'\n",
      "Superfluous whitespace found in object header b'22' b'0'\n",
      "Superfluous whitespace found in object header b'32' b'0'\n",
      "Superfluous whitespace found in object header b'31' b'0'\n",
      "Superfluous whitespace found in object header b'30' b'0'\n",
      "Superfluous whitespace found in object header b'41' b'0'\n",
      "Superfluous whitespace found in object header b'40' b'0'\n",
      "Superfluous whitespace found in object header b'39' b'0'\n",
      "Superfluous whitespace found in object header b'42' b'0'\n",
      "Superfluous whitespace found in object header b'16' b'0'\n",
      "Superfluous whitespace found in object header b'33' b'0'\n",
      "Superfluous whitespace found in object header b'47' b'0'\n",
      "Superfluous whitespace found in object header b'46' b'0'\n",
      "Superfluous whitespace found in object header b'45' b'0'\n",
      "Superfluous whitespace found in object header b'51' b'0'\n",
      "Superfluous whitespace found in object header b'50' b'0'\n",
      "Superfluous whitespace found in object header b'49' b'0'\n",
      "Superfluous whitespace found in object header b'54' b'0'\n",
      "Superfluous whitespace found in object header b'53' b'0'\n",
      "Superfluous whitespace found in object header b'91' b'0'\n",
      "Superfluous whitespace found in object header b'63' b'0'\n",
      "Superfluous whitespace found in object header b'62' b'0'\n",
      "Superfluous whitespace found in object header b'61' b'0'\n",
      "Superfluous whitespace found in object header b'89' b'0'\n",
      "Superfluous whitespace found in object header b'88' b'0'\n",
      "Superfluous whitespace found in object header b'87' b'0'\n",
      "Superfluous whitespace found in object header b'90' b'0'\n",
      "Superfluous whitespace found in object header b'81' b'0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'23' b'0'\n",
      "Superfluous whitespace found in object header b'35' b'0'\n",
      "Superfluous whitespace found in object header b'46' b'0'\n",
      "Superfluous whitespace found in object header b'49' b'0'\n",
      "Superfluous whitespace found in object header b'52' b'0'\n",
      "Superfluous whitespace found in object header b'71' b'0'\n",
      "Superfluous whitespace found in object header b'83' b'0'\n",
      "Superfluous whitespace found in object header b'22' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'20' b'0'\n",
      "Superfluous whitespace found in object header b'19' b'0'\n",
      "Superfluous whitespace found in object header b'18' b'0'\n",
      "Superfluous whitespace found in object header b'21' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'34' b'0'\n",
      "Superfluous whitespace found in object header b'32' b'0'\n",
      "Superfluous whitespace found in object header b'31' b'0'\n",
      "Superfluous whitespace found in object header b'30' b'0'\n",
      "Superfluous whitespace found in object header b'33' b'0'\n",
      "Superfluous whitespace found in object header b'24' b'0'\n",
      "Superfluous whitespace found in object header b'45' b'0'\n",
      "Superfluous whitespace found in object header b'43' b'0'\n",
      "Superfluous whitespace found in object header b'42' b'0'\n",
      "Superfluous whitespace found in object header b'41' b'0'\n",
      "Superfluous whitespace found in object header b'44' b'0'\n",
      "Superfluous whitespace found in object header b'48' b'0'\n",
      "Superfluous whitespace found in object header b'47' b'0'\n",
      "Superfluous whitespace found in object header b'51' b'0'\n",
      "Superfluous whitespace found in object header b'50' b'0'\n",
      "Superfluous whitespace found in object header b'70' b'0'\n",
      "Superfluous whitespace found in object header b'60' b'0'\n",
      "Superfluous whitespace found in object header b'59' b'0'\n",
      "Superfluous whitespace found in object header b'58' b'0'\n",
      "Superfluous whitespace found in object header b'68' b'0'\n",
      "Superfluous whitespace found in object header b'67' b'0'\n",
      "Superfluous whitespace found in object header b'66' b'0'\n",
      "Superfluous whitespace found in object header b'69' b'0'\n",
      "Superfluous whitespace found in object header b'82' b'0'\n",
      "Superfluous whitespace found in object header b'79' b'0'\n",
      "Superfluous whitespace found in object header b'78' b'0'\n",
      "Superfluous whitespace found in object header b'77' b'0'\n",
      "Superfluous whitespace found in object header b'81' b'0'\n",
      "Superfluous whitespace found in object header b'80' b'0'\n",
      "Superfluous whitespace found in object header b'86' b'0'\n",
      "Superfluous whitespace found in object header b'85' b'0'\n",
      "Superfluous whitespace found in object header b'84' b'0'\n",
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'23' b'0'\n",
      "Superfluous whitespace found in object header b'35' b'0'\n",
      "Superfluous whitespace found in object header b'46' b'0'\n",
      "Superfluous whitespace found in object header b'49' b'0'\n",
      "Superfluous whitespace found in object header b'52' b'0'\n",
      "Superfluous whitespace found in object header b'71' b'0'\n",
      "Superfluous whitespace found in object header b'83' b'0'\n",
      "Superfluous whitespace found in object header b'22' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'20' b'0'\n",
      "Superfluous whitespace found in object header b'19' b'0'\n",
      "Superfluous whitespace found in object header b'18' b'0'\n",
      "Superfluous whitespace found in object header b'21' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'34' b'0'\n",
      "Superfluous whitespace found in object header b'32' b'0'\n",
      "Superfluous whitespace found in object header b'31' b'0'\n",
      "Superfluous whitespace found in object header b'30' b'0'\n",
      "Superfluous whitespace found in object header b'33' b'0'\n",
      "Superfluous whitespace found in object header b'24' b'0'\n",
      "Superfluous whitespace found in object header b'45' b'0'\n",
      "Superfluous whitespace found in object header b'43' b'0'\n",
      "Superfluous whitespace found in object header b'42' b'0'\n",
      "Superfluous whitespace found in object header b'41' b'0'\n",
      "Superfluous whitespace found in object header b'44' b'0'\n",
      "Superfluous whitespace found in object header b'48' b'0'\n",
      "Superfluous whitespace found in object header b'47' b'0'\n",
      "Superfluous whitespace found in object header b'51' b'0'\n",
      "Superfluous whitespace found in object header b'50' b'0'\n",
      "Superfluous whitespace found in object header b'70' b'0'\n",
      "Superfluous whitespace found in object header b'60' b'0'\n",
      "Superfluous whitespace found in object header b'59' b'0'\n",
      "Superfluous whitespace found in object header b'58' b'0'\n",
      "Superfluous whitespace found in object header b'68' b'0'\n",
      "Superfluous whitespace found in object header b'67' b'0'\n",
      "Superfluous whitespace found in object header b'66' b'0'\n",
      "Superfluous whitespace found in object header b'69' b'0'\n",
      "Superfluous whitespace found in object header b'82' b'0'\n",
      "Superfluous whitespace found in object header b'79' b'0'\n",
      "Superfluous whitespace found in object header b'78' b'0'\n",
      "Superfluous whitespace found in object header b'77' b'0'\n",
      "Superfluous whitespace found in object header b'81' b'0'\n",
      "Superfluous whitespace found in object header b'80' b'0'\n",
      "Superfluous whitespace found in object header b'86' b'0'\n",
      "Superfluous whitespace found in object header b'85' b'0'\n",
      "Superfluous whitespace found in object header b'84' b'0'\n",
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'23' b'0'\n",
      "Superfluous whitespace found in object header b'35' b'0'\n",
      "Superfluous whitespace found in object header b'46' b'0'\n",
      "Superfluous whitespace found in object header b'49' b'0'\n",
      "Superfluous whitespace found in object header b'52' b'0'\n",
      "Superfluous whitespace found in object header b'71' b'0'\n",
      "Superfluous whitespace found in object header b'83' b'0'\n",
      "Superfluous whitespace found in object header b'22' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'20' b'0'\n",
      "Superfluous whitespace found in object header b'19' b'0'\n",
      "Superfluous whitespace found in object header b'18' b'0'\n",
      "Superfluous whitespace found in object header b'21' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'34' b'0'\n",
      "Superfluous whitespace found in object header b'32' b'0'\n",
      "Superfluous whitespace found in object header b'31' b'0'\n",
      "Superfluous whitespace found in object header b'30' b'0'\n",
      "Superfluous whitespace found in object header b'33' b'0'\n",
      "Superfluous whitespace found in object header b'24' b'0'\n",
      "Superfluous whitespace found in object header b'45' b'0'\n",
      "Superfluous whitespace found in object header b'43' b'0'\n",
      "Superfluous whitespace found in object header b'42' b'0'\n",
      "Superfluous whitespace found in object header b'41' b'0'\n",
      "Superfluous whitespace found in object header b'44' b'0'\n",
      "Superfluous whitespace found in object header b'48' b'0'\n",
      "Superfluous whitespace found in object header b'47' b'0'\n",
      "Superfluous whitespace found in object header b'51' b'0'\n",
      "Superfluous whitespace found in object header b'50' b'0'\n",
      "Superfluous whitespace found in object header b'70' b'0'\n",
      "Superfluous whitespace found in object header b'60' b'0'\n",
      "Superfluous whitespace found in object header b'59' b'0'\n",
      "Superfluous whitespace found in object header b'58' b'0'\n",
      "Superfluous whitespace found in object header b'68' b'0'\n",
      "Superfluous whitespace found in object header b'67' b'0'\n",
      "Superfluous whitespace found in object header b'66' b'0'\n",
      "Superfluous whitespace found in object header b'69' b'0'\n",
      "Superfluous whitespace found in object header b'82' b'0'\n",
      "Superfluous whitespace found in object header b'79' b'0'\n",
      "Superfluous whitespace found in object header b'78' b'0'\n",
      "Superfluous whitespace found in object header b'77' b'0'\n",
      "Superfluous whitespace found in object header b'81' b'0'\n",
      "Superfluous whitespace found in object header b'80' b'0'\n",
      "Superfluous whitespace found in object header b'86' b'0'\n",
      "Superfluous whitespace found in object header b'85' b'0'\n",
      "Superfluous whitespace found in object header b'84' b'0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 70):\n",
    "    print(i)\n",
    "    download_pdf(ids[i], courseletlist[i])\n",
    "    pdf_output, totalpages = cleaning(ids[i])\n",
    "    word_list_stemmed, one_word_list, two_word_list, two_word_permutation_list, two_word_permutation_set, three_word_list, three_word_permutation_list, three_word_permutation_set = word_lists(ids[i])\n",
    "    dictionary_one_word_0 = occurance_one_matrix_creator(ids[i])\n",
    "    [dictionary_one_word.append(j) for j in dictionary_one_word_0]\n",
    "    combined_pdf.append(' '.join(pdf_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_one = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, use_idf=True, lowercase = False, stop_words = stopwords_list, ngram_range=(1,1))\n",
    "X_one = vectorizer_one.fit_transform(combined_pdf)\n",
    "\n",
    "vectorizer_two = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, use_idf=True, lowercase = False, stop_words = stopwords_list, ngram_range=(2,2))\n",
    "X_two = vectorizer_two.fit_transform(combined_pdf)\n",
    "\n",
    "vectorizer_three = TfidfVectorizer(smooth_idf=True, sublinear_tf=True, use_idf=True, lowercase = False, stop_words = stopwords_list, ngram_range=(3,3))\n",
    "X_three = vectorizer_three.fit_transform(combined_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>id-1</th>\n",
       "      <th>id-2</th>\n",
       "      <th>id-3</th>\n",
       "      <th>id-4</th>\n",
       "      <th>id-5</th>\n",
       "      <th>id-6</th>\n",
       "      <th>id-7</th>\n",
       "      <th>id-8</th>\n",
       "      <th>id-9</th>\n",
       "      <th>id-10</th>\n",
       "      <th>...</th>\n",
       "      <th>id-61</th>\n",
       "      <th>id-62</th>\n",
       "      <th>id-63</th>\n",
       "      <th>id-64</th>\n",
       "      <th>id-65</th>\n",
       "      <th>id-66</th>\n",
       "      <th>id-67</th>\n",
       "      <th>id-68</th>\n",
       "      <th>id-69</th>\n",
       "      <th>id-70</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaab</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaac</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaacchicbvfna</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaacehicbvfnbxmxepuuxyv</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaack</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zytdnt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zytjtvt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzl</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12404 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id                       id-1  id-2  id-3  id-4  id-5  id-6  id-7  id-8  id-9  \\\n",
       "aaab                      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "aaac                      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "aaacchicbvfna             0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "aaacehicbvfnbxmxepuuxyv   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "aaack                     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...                       ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "zyt                       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zytdnt                    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zytjtvt                   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zzl                       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "zzt                       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "id                       id-10  ...  id-61  id-62  id-63  id-64  id-65  id-66  \\\n",
       "aaab                       0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "aaac                       0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "aaacchicbvfna              0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "aaacehicbvfnbxmxepuuxyv    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "aaack                      0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...                        ...  ...    ...    ...    ...    ...    ...    ...   \n",
       "zyt                        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "zytdnt                     0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "zytjtvt                    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "zzl                        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "zzt                        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "id                       id-67  id-68  id-69  id-70  \n",
       "aaab                       0.0    0.0    0.0    0.0  \n",
       "aaac                       0.0    0.0    0.0    0.0  \n",
       "aaacchicbvfna              0.0    0.0    0.0    0.0  \n",
       "aaacehicbvfnbxmxepuuxyv    0.0    0.0    0.0    0.0  \n",
       "aaack                      0.0    0.0    0.0    0.0  \n",
       "...                        ...    ...    ...    ...  \n",
       "zyt                        0.0    0.0    0.0    0.0  \n",
       "zytdnt                     0.0    0.0    0.0    0.0  \n",
       "zytjtvt                    0.0    0.0    0.0    0.0  \n",
       "zzl                        0.0    0.0    0.0    0.0  \n",
       "zzt                        0.0    0.0    0.0    0.0  \n",
       "\n",
       "[12404 rows x 70 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS WORKS\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(smooth_idf=True, sublinear_tf=True, use_idf=True, lowercase = False, stop_words = stopwords_list, ngram_range=(1,1)) \n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(combined_pdf)\n",
    "\n",
    "matrix_tfidfvectorizer=tfidf_vectorizer_vectors\n",
    "\n",
    "df = pd.DataFrame(matrix_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=id_matrix['id']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The tuple represents: (document_id, token_id)\n",
    "# # The value following the tuple represents the tf-idf score of a given token in a given document\n",
    "# # The tuples that are not there have a tf-idf score of 0\n",
    "# # vectorizer_one.get_feature_names_out()[5278]\n",
    "\n",
    "# print(X_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_one = pd.DataFrame(X_one.toarray(), columns = \n",
    "vectorizer_one.get_feature_names_out())\n",
    "\n",
    "xx_two = pd.DataFrame(X_two.toarray(), columns = \n",
    "vectorizer_two.get_feature_names_out())\n",
    "\n",
    "xx_three = pd.DataFrame(X_three.toarray(), columns = \n",
    "vectorizer_three.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_one.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_obj = TruncatedSVD(n_components=30)\n",
    "tfidf_lsa_data_one = lsa_obj.fit_transform(X_one)\n",
    "Sigma_one = lsa_obj.singular_values_\n",
    "V_T_one = lsa_obj.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_obj = TruncatedSVD(n_components=30)\n",
    "tfidf_lsa_data_two = lsa_obj.fit_transform(X_two)\n",
    "Sigma_two = lsa_obj.singular_values_\n",
    "V_T_two = lsa_obj.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_obj = TruncatedSVD(n_components=30)\n",
    "tfidf_lsa_data_three = lsa_obj.fit_transform(X_three)\n",
    "Sigma_three = lsa_obj.singular_values_\n",
    "V_T_three = lsa_obj.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33959657, -0.13252616, -0.05815999, ...,  0.17129969,\n",
       "         0.3206116 , -0.08372254],\n",
       "       [ 0.25662924, -0.24386218, -0.27344773, ...,  0.08320862,\n",
       "         0.06140737,  0.13423528],\n",
       "       [ 0.29381235, -0.24360558, -0.28536261, ...,  0.02073898,\n",
       "         0.06572342,  0.04026229],\n",
       "       ...,\n",
       "       [ 0.33106741,  0.02280927,  0.07490754, ...,  0.04092529,\n",
       "         0.13280786,  0.14077805],\n",
       "       [ 0.2137848 ,  0.17835266, -0.10624062, ..., -0.0534932 ,\n",
       "         0.0832899 ,  0.0930224 ],\n",
       "       [ 0.24011993,  0.21843181, -0.10298943, ...,  0.0227846 ,\n",
       "        -0.02774666, -0.05101788]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates U\n",
    "tfidf_lsa_data_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.68216527, 1.51055267, 1.47513271, 1.37237082, 1.32592356,\n",
       "       1.31453848, 1.2952371 , 1.26685274, 1.24929095, 1.20949671,\n",
       "       1.1627485 , 1.14200215, 1.13024925, 1.11323477, 1.05442694,\n",
       "       1.05194955, 1.04532784, 1.01175333, 1.00814593, 0.99494734,\n",
       "       0.99105862, 0.98262074, 0.98170026, 0.97602317, 0.97102693,\n",
       "       0.96989151, 0.95616646, 0.94797109, 0.94196336, 0.93425204])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate Sigma.\n",
    "# The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space.\n",
    "Sigma_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.91590711e-03,  1.10663720e-03,  8.56310668e-04, ...,\n",
       "         2.74643419e-03,  8.56310668e-04,  1.56437959e-03],\n",
       "       [-2.04410374e-03,  2.95229871e-04,  2.04274091e-04, ...,\n",
       "        -4.17920678e-03,  2.04274091e-04, -2.22267046e-04],\n",
       "       [-1.39331620e-02, -1.46316662e-03, -1.12264527e-03, ...,\n",
       "         7.41821542e-03, -1.12264527e-03, -1.00700063e-04],\n",
       "       ...,\n",
       "       [ 2.21854721e-03,  5.15023594e-03, -1.22828517e-04, ...,\n",
       "        -9.93843487e-05, -1.22828517e-04,  1.60894141e-02],\n",
       "       [ 2.46030891e-03,  1.20583085e-03,  4.40848403e-04, ...,\n",
       "         2.61058391e-03,  4.40848403e-04, -3.76358168e-05],\n",
       "       [ 5.98088826e-03, -2.72445927e-03, -4.68474192e-03, ...,\n",
       "         1.15396922e-03, -4.68474192e-03,  1.49954118e-02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The right singular vectors of the input data.\n",
    "lsa_obj.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.91590711e-03, -2.04410374e-03, -1.39331620e-02, ...,\n",
       "         2.21854721e-03,  2.46030891e-03,  5.98088826e-03],\n",
       "       [ 1.10663720e-03,  2.95229871e-04, -1.46316662e-03, ...,\n",
       "         5.15023594e-03,  1.20583085e-03, -2.72445927e-03],\n",
       "       [ 8.56310668e-04,  2.04274091e-04, -1.12264527e-03, ...,\n",
       "        -1.22828517e-04,  4.40848403e-04, -4.68474192e-03],\n",
       "       ...,\n",
       "       [ 2.74643419e-03, -4.17920678e-03,  7.41821542e-03, ...,\n",
       "        -9.93843487e-05,  2.61058391e-03,  1.15396922e-03],\n",
       "       [ 8.56310668e-04,  2.04274091e-04, -1.12264527e-03, ...,\n",
       "        -1.22828517e-04,  4.40848403e-04, -4.68474192e-03],\n",
       "       [ 1.56437959e-03, -2.22267046e-04, -1.00700063e-04, ...,\n",
       "         1.60894141e-02, -3.76358168e-05,  1.49954118e-02]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates Sgenerates V_T\n",
    "V_T_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsa_obj.explained_variance_ratio_)\n",
    "\n",
    "print(lsa_obj.explained_variance_ratio_.sum())\n",
    "\n",
    "print(lsa_obj.singular_values_)\n",
    "\n",
    "print(lsa_obj.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# ax = sns.barplot(x=lsa_obj.feature_names_in_[0:20], y = Sigma[0:20])\n",
    "# ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_topic_matrix = pd.DataFrame(data=lsa_term_topic, \n",
    "#                                  index = eda_train.columns, \n",
    "#                                  columns = [f'Latent_concept_{r}' for r in range(0,V_T.shape[1])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_one = 'pricing'\n",
    "input_one = input_one.split(' ')\n",
    "input_one = [i for i in input_one if i not in stopwords_list]\n",
    "input_one = [ps.stem(i) for i in input_one]\n",
    "\n",
    "input_two = 'arbitrage market'\n",
    "input_two = input_two.split(' ')\n",
    "input_two = [i for i in input_two if i not in stopwords_list]\n",
    "input_two = [ps.stem(i) for i in input_two]\n",
    "input_two = [' '.join(input_two)]\n",
    "\n",
    "input_three = 'statistics of financial markets'\n",
    "input_three = input_three.split(' ')\n",
    "input_three = [i for i in input_three if i not in stopwords_list]\n",
    "input_three = [ps.stem(i) for i in input_three]\n",
    "input_three = [' '.join(input_three)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F186%2F1645194357Presentation_Quantinar_with_videos.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F175%2F163757167020201210_Liu_crypto_p2p_lending.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F272%2F1654160257Lesson1-1.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F281%2F1654251525Lesson2-3.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F103%2F20210303+IA+METIS+Reinforcement+Learning.pdf']\n"
     ]
    }
   ],
   "source": [
    "if input_one[0] in list(xx_one.columns):\n",
    "    ranked_indexes_one = xx_one[input_one].sort_values(by = input_one, ascending=False).index\n",
    "    ranked_indexes_one = list(ranked_indexes_one[0:5])\n",
    "    output_one = [courseletlist[i] for i in ranked_indexes_one]\n",
    "    print(output_one)\n",
    "else: \n",
    "    print('not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F175%2F163757167020201210_Liu_crypto_p2p_lending.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F272%2F1654160257Lesson1-1.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F196%2F1642599075163458186420200403+METIS+Kho+Hae+Spectral+Clustering+course.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F181%2F163826827620211130+LI+Hae+Case+based+Bancruptcy+prediction.pdf', 'https://quantinar.s3.eu-west-3.amazonaws.com/courselet_components%2F306%2FBarHan2021_talk.pdf']\n"
     ]
    }
   ],
   "source": [
    "if input_two[0] in list(xx_two.columns):\n",
    "    ranked_indexes_two = xx_two[input_two].sort_values(by = input_two, ascending=False).index\n",
    "    ranked_indexes_two = list(ranked_indexes_two[0:5])\n",
    "    output_two = [courseletlist[i] for i in ranked_indexes_two]\n",
    "    print(output_two)\n",
    "else: \n",
    "    print('not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not present\n"
     ]
    }
   ],
   "source": [
    "if input_three[0] in list(xx_three.columns):\n",
    "    ranked_indexes_three = xx_three[input_three].sort_values(by = input_three, ascending=False).index\n",
    "    ranked_indexes_three = list(ranked_indexes_three[0:5])\n",
    "    output_three = [courseletlist[i] for i in ranked_indexes_three]\n",
    "    print(output_three)\n",
    "else: \n",
    "    print('not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
